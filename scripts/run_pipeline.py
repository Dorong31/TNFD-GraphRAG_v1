"""
TNFD-GraphRAG ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ ìŠ¤í¬ë¦½íŠ¸

PDF ë¬¸ì„œì—ì„œ Knowledge Graph Tripleì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
ì§„í–‰ë¥  í‘œì‹œ, ì¤‘ê°„ ì €ì¥, ì¬ê°œ ê¸°ëŠ¥ì„ ì§€ì›í•©ë‹ˆë‹¤.

ì‚¬ìš© ì˜ˆì‹œ:
  # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
  python scripts/run_pipeline.py data/pdfs/report.pdf

  # ì²˜ìŒ 5ê°œ ì²­í¬ë§Œ í…ŒìŠ¤íŠ¸
  python scripts/run_pipeline.py data/pdfs/report.pdf --limit 5

  # 10ë²ˆì§¸ ì²­í¬ë¶€í„° ì‹œì‘ (ì´ì „ ì‹¤í–‰ ì´ì–´ì„œ)
  python scripts/run_pipeline.py data/pdfs/report.pdf --start 10

  # 20~30ë²ˆ ì²­í¬ë§Œ ì²˜ë¦¬
  python scripts/run_pipeline.py data/pdfs/report.pdf --start 20 --limit 10
"""

import os
import sys
import json
import time
import argparse
from pathlib import Path

# src íŒ¨í‚¤ì§€ë¥¼ import ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

from src.data_pipeline.pdf_loader import load_pdf
from src.data_pipeline.chunker import create_chunks_from_pages
from src.extraction.extractor import TripleExtractor


def save_results(results, output_path):
    """
    ì¶”ì¶œ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        results: ExtractionResult ë¦¬ìŠ¤íŠ¸
        output_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
    """
    output_data = []
    for res in results:
        # Pydantic ëª¨ë¸ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        nodes_data = [node.model_dump() for node in res.nodes]
        rels_data = [rel.model_dump() for rel in res.relationships]
        
        output_data.append({
            "source_evidence_id": res.source_evidence_id,
            "nodes": nodes_data,
            "relationships": rels_data
        })

    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)


def run_pipeline(pdf_path, output_dir, limit=None, start=0):
    """
    PDFì—ì„œ Triple ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    
    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        limit: ì²˜ë¦¬í•  ìµœëŒ€ ì²­í¬ ìˆ˜ (Noneì´ë©´ ì „ì²´)
        start: ì‹œì‘í•  ì²­í¬ ì¸ë±ìŠ¤ (0ë¶€í„°)
    """
    pdf_stem = Path(pdf_path).stem
    
    # ====== 1ë‹¨ê³„: PDF ë¡œë“œ ======
    print(f"ğŸ“„ PDF ë¡œë“œ ì¤‘... ({pdf_path})")
    try:
        pages = load_pdf(pdf_path)
    except Exception as e:
        print(f"âŒ PDF ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    print(f"   âœ“ {len(pages)} í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")

    # ====== 2ë‹¨ê³„: í…ìŠ¤íŠ¸ ì²­í‚¹ ======
    print(f"ğŸ“ í…ìŠ¤íŠ¸ ì²­í‚¹ ì¤‘...")
    chunks = create_chunks_from_pages(pages)
    total_chunks = len(chunks)
    print(f"   âœ“ {total_chunks} ì²­í¬ ìƒì„± ì™„ë£Œ")

    # ì‹œì‘/ì œí•œ ë²”ìœ„ ì ìš©
    end = total_chunks
    if limit:
        end = min(start + limit, total_chunks)
    
    chunks = chunks[start:end]
    process_count = len(chunks)
    
    if start > 0 or limit:
        print(f"   â†’ ì²­í¬ ë²”ìœ„: [{start}:{end}] ({process_count}ê°œ ì²˜ë¦¬)")

    # ====== 3ë‹¨ê³„: Triple ì¶”ì¶œ ======
    print(f"\nğŸ” Triple ì¶”ì¶œ ì‹œì‘...")
    print(f"   (LLM APIë¥¼ í˜¸ì¶œí•˜ë¯€ë¡œ ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤)")
    print(f"   {'â”€' * 50}")
    
    extractor = TripleExtractor()
    results = []
    start_time = time.time()
    
    # ì¤‘ê°„ ì €ì¥ ê²½ë¡œ ì„¤ì •
    output_path = Path(output_dir) / f"{pdf_stem}_extraction.json"
    partial_path = Path(output_dir) / f"{pdf_stem}_extraction_partial.json"
    
    for i, chunk in enumerate(chunks):
        chunk_start = time.time()
        global_idx = start + i  # ì „ì²´ ê¸°ì¤€ ì¸ë±ìŠ¤
        
        # ì§„í–‰ë¥  í‘œì‹œ
        elapsed = time.time() - start_time
        if i > 0:
            avg_per_chunk = elapsed / i
            remaining = avg_per_chunk * (process_count - i)
            eta_str = f"ë‚¨ì€ ì˜ˆìƒ: {remaining/60:.1f}ë¶„"
        else:
            eta_str = "ê³„ì‚° ì¤‘..."
        
        print(f"   [{i+1}/{process_count}] ì²­í¬ #{global_idx} "
              f"(p.{chunk.get('page_num', '?')}) ì²˜ë¦¬ ì¤‘... ", end="", flush=True)
        
        # ì¶”ì¶œ ì‹¤í–‰
        result = extractor.extract(
            text=chunk.get("text", ""),
            source_doc=chunk.get("source_doc", ""),
            page_num=chunk.get("page_num", 0),
            chunk_index=chunk.get("chunk_index", i)
        )
        results.append(result)
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        chunk_time = time.time() - chunk_start
        node_count = len(result.nodes)
        rel_count = len(result.relationships)
        print(f"ë…¸ë“œ {node_count}, ê´€ê³„ {rel_count} "
              f"({chunk_time:.1f}ì´ˆ) | {eta_str}")
        
        # ë§¤ ì²­í¬ë§ˆë‹¤ ì¤‘ê°„ ì €ì¥ (crash ë°©ì§€)
        save_results(results, partial_path)

    # ====== 4ë‹¨ê³„: ìµœì¢… ì €ì¥ ======
    elapsed_total = time.time() - start_time
    
    # ìµœì¢… íŒŒì¼ë¡œ ì €ì¥
    save_results(results, output_path)
    
    # ì¤‘ê°„ ì €ì¥ íŒŒì¼ ì‚­ì œ
    if partial_path.exists():
        partial_path.unlink()

    # ====== ì™„ë£Œ ìš”ì•½ ======
    print(f"\n{'â•' * 50}")
    print(f"âœ… ì¶”ì¶œ ì™„ë£Œ!")
    print(f"{'â•' * 50}")
    
    total_nodes = sum(len(res.nodes) for res in results)
    total_rels = sum(len(res.relationships) for res in results)
    empty_chunks = sum(1 for res in results if len(res.nodes) == 0)
    
    print(f"   ğŸ“Š ì´ ë…¸ë“œ: {total_nodes}")
    print(f"   ğŸ“Š ì´ ê´€ê³„: {total_rels}")
    print(f"   ğŸ“Š ë¹ˆ ì²­í¬: {empty_chunks}/{process_count}")
    print(f"   â±ï¸  ì†Œìš” ì‹œê°„: {elapsed_total/60:.1f}ë¶„ "
          f"(ì²­í¬ í‰ê· : {elapsed_total/max(process_count, 1):.1f}ì´ˆ)")
    print(f"   ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="TNFD-GraphRAG Triple ì¶”ì¶œ íŒŒì´í”„ë¼ì¸",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python scripts/run_pipeline.py data/pdfs/report.pdf
  python scripts/run_pipeline.py data/pdfs/report.pdf --limit 5
  python scripts/run_pipeline.py data/pdfs/report.pdf --start 10 --limit 5
        """
    )
    parser.add_argument("pdf_path", help="ì²˜ë¦¬í•  PDF íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--limit", type=int, help="ì²˜ë¦¬í•  ìµœëŒ€ ì²­í¬ ìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)")
    parser.add_argument("--start", type=int, default=0, help="ì‹œì‘í•  ì²­í¬ ì¸ë±ìŠ¤ (ê¸°ë³¸: 0)")
    parser.add_argument("--output", default="output", help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: output)")
    
    args = parser.parse_args()
    
    run_pipeline(args.pdf_path, args.output, args.limit, args.start)
